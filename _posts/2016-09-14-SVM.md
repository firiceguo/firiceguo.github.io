---
layout: post
title: "Support Vector Machine (SVM)"
date: 2016-09-14 15:01
tags: MachineLearning
categories: MachineLearning
description: Questions about SVM
---

## About SVM

### 为什么要出现SVM这个算法？

二维平面中线性不可分的二分类问题，如`XOR`问题，为了解决这类问题，就出现了支持向量机算法（SVM）。

### 为什么是SVM这样的设计？

在优化过程中，遵循一些哲学上的基本原则：

- [奥卡姆剃刀原则 (Occam's razor)](https://en.wikipedia.org/wiki/Occam%27s_razor)：如果关于同一个问题有许多种理论，每一种都能作出同样准确的预言，那么应该挑选其中使用假定最少的。尽管越复杂的方法通常能作出越好的预言，但是在不考虑预言能力的情况下，前提假设越少越好。

- [VC维理论(Vapnik–Chervonenkis dimension)](https://en.wikipedia.org/wiki/VC_dimension)

- [最小描述长度(minimum description length (MDL) principle)](https://en.wikipedia.org/wiki/Minimum_description_length)：奥卡姆剃刀形式化的一个结果

- [偏差和方差之间的权衡](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)；[一致收敛](https://en.wikipedia.org/wiki/Uniform_convergence)

- [维数灾难](https://en.wikipedia.org/wiki/Curse_of_dimensionality)：当维数提高时，空间的体积提高太快，因而可用数据变得很稀疏。

### 什么是间隔（Margin）？

能把要分类的两类样本分开的超平面有很多个，直观上来说，位于两类训练样本“正中间”的超平面对于错误的容忍度最高。间隔的定义为：

![chart]({{ site.baseurl | prepend:site.url}}/images/margin.png){: .center-image }

### 什么是核函数？

当线性不可分问题出现时，将其映射到高维后，一定会有一个超平面将两类样本分开。

假设映射函数为`y(x)`，由于计算过程中需要计算`y(xi)^T*y(xj)`，由于直接计算比较复杂，假设一个函数`k(xi,xj) = <y(xi),y(xj)> = y(xi)^T*y(xj)`来代替`xi`和`xj`在特征空间的内积，这里的`k(·,·)`即为核函数。

### 如何选择核函数？

核函数有线性核、高斯核等等，一般先尝试高斯核。

[这个](http://playground.tensorflow.org)网站上面可以针对神经网络算法进行实验。
