---
layout: post
title: "Support Vector Machine (SVM)"
date: 2016-09-14 15:01
tags: MachineLearning
categories: MachineLearning
thumbnail:  book
description: Questions about SVM
---

## About SVM

### 为什么要出现SVM这个算法？

在二维可分平面中，会有很多分界线，但是哪个是最好的，SVM算法给出了结果，margin最大的是最好的。（Linear SVM, LSVM）

二维平面中线性不可分的二分类问题，如`XOR`问题，为了解决这类问题，就出现了支持向量机算法（SVM）。

### 为什么是SVM这样的设计？

在优化过程中，遵循一些哲学上的基本原则：

- [奥卡姆剃刀原则 (Occam's razor)](https://en.wikipedia.org/wiki/Occam%27s_razor)：如果关于同一个问题有许多种理论，每一种都能作出同样准确的预言，那么应该挑选其中使用假定最少的。尽管越复杂的方法通常能作出越好的预言，但是在不考虑预言能力的情况下，前提假设越少越好。在SVM中，就是feature的数量越少越好。

> 当需要更多features的时候，deep learning出现了。

- [VC维理论(Vapnik–Chervonenkis dimension)](https://en.wikipedia.org/wiki/VC_dimension)

- [最小描述长度(minimum description length (MDL) principle)](https://en.wikipedia.org/wiki/Minimum_description_length)：奥卡姆剃刀形式化的一个结果

- [偏差和方差之间的权衡](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)；[一致收敛](https://en.wikipedia.org/wiki/Uniform_convergence)

- [维数灾难](https://en.wikipedia.org/wiki/Curse_of_dimensionality)：当维数提高时，空间的体积提高太快，因而可用数据变得很稀疏。

### 什么是间隔（Margin）？

能把要分类的两类样本分开的超平面有很多个，直观上来说，位于两类训练样本“正中间”的超平面对于错误的容忍度最高。间隔的定义为：

$$ \gamma = \min_i \frac{|f_{w,b}(x_i)|}{||w||} $$

### soft margin？

允许“错误”$$ \xi_i $$发生，那么我们的判定就会变成这样：

$$ \left\{ {w^Tx_i+b\ge {1-\xi_i}\qquad y_i=1} \atop{w^Tx_i+b\le  {-1+\xi_i}\quad y_i=-1} \atop{\xi \ge 0 \quad \forall  i} \right $$

其中$$\xi_i$$被称为“松弛变量”（slack variable）

所以原来的目标函数就会变成这样：

$$ \min\frac 1 2 ||w||^2 + C \sum_{i=1}^n\xi_i $$

其中C是一个参数，用于控制目标函数中两项（“寻找margin最大的超平面”和“保证数据点偏差量最小”）之间的权重。

### 什么是核函数？

当线性不可分问题出现时，将其映射到高维后，一定会有一个超平面将两类样本分开。

假设映射函数为 $$ y(x) $$，由于计算过程中需要计算 $$ y(x_i)^T*y(x_j) $$，由于直接计算比较复杂，并且维度很高。

假设一个函数 $$ k(xi,xj) = \left\langle y(x_i),y(x_j) \right\rangle = y(x_i)^T*y(x_j) $$ 来代替 $$ x_i $$ 和 $$ x_j $$在特征空间的内积，这里的 $$ k(·,·) $$ 即为核函数。

### 如何选择核函数？

核函数有线性核、高斯核等等，一般先尝试高斯核。

[这个](http://playground.tensorflow.org)网站上面可以针对神经网络算法进行实验。
