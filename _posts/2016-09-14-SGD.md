---
layout: post
title: "Stochastic gradient descent (SGD)"
date: 2016-09-14 13:39
tags: MachineLearning
categories: MachineLearning
thumbnail:  book
description: Questions about SGD
---

## About SGD

### 什么是Gradient Descent(GD)？

梯度下降法(GD)是一种寻找使误差函数最小化的方法。

梯度下降的公式如下：

$$ \theta_{t+1} = \theta_{t} - \eta_t \nabla\hat{L}(\theta_{t}) $$

其中的$$ \eta_t $$称为学习速率

### 为什么要用SGD？

假设我们的数据是按照时间线获取的，那么我们就不知道总体的误差函数，我们只知道当前时间`t`的误差函数，也就是总体误差函数的估计，那么我们就按照这个估计来进行计算：

$$ \theta_{t+1} = \theta_{t} - \eta_t \nabla l(\theta_{t},x_t,y_t) $$

实际上，SGD就是随机抽取一部分训练集代替整个训练集，这样会使计算速度变快。在上面的公式中是用时间`t`来设计抽取的训练集的。

> 查阅资料发现，是前期速度变快，但是精度不高，因此针对梯度下降还有很多优化方案，如批量梯度下降(BGD)，小批量梯度下降(MBGD)等。

### SGD怎么用？

- linear regression

$$ w_{t+1} = w_{t} - \eta_t \nabla l(w_{t},x_t,y_t) = w_t-\frac{2\eta_t}n(w_t^Tx_t-y_t)x_t $$

- logistic regression

$$ w_{t+1} = w_{t} - \eta_t \nabla l(w_{t},x_t,y_t) = w_t-\frac{\eta_t}n \frac{\sigma (a)(1-\sigma (a))}{\sigma (a)}y_tx_t $$

其中：

$$ a=y_t w_t^T x_t $$

- Perceptron

$$ w_{t+1} = w_{t} - \eta_t \nabla l(w_{t},x_t,y_t) = w_t+\eta_ty_tx_t \prod[\text{mistake on }x_t] $$

### SGD的特点？

优点：

- 可适用范围广

- 在大部分情况下容易实现

- 对很多情况都能保证误差范围

- 在误差、时间、内存等方面表现良好

缺点：

- 对于非凸函数不能保证找到最优解（在深度学习中可以找到）

- 超参数（就是这些参数需要在validation set到达最佳的时候来确定大小）：初始化和学习速率

### Mini-batch

小批量梯度下降的思路是通过小批量的点代替单个点。

$$ \theta _{t+1} = \theta _{t} - \eta_t \nabla (\frac1b\sum_{1\le i\le b}l(\theta_t,x_{tb+i},y_{tb+i})) $$
