---
layout: post
title: "A little web crawl"
date: 2016-09-27 18:24
tags: 
    - web
    - python
categories: 
    - web
    - python
thumbnail: bug
comments: true
description: A little web crawl concurrence support
---

## 网络爬虫（web crawl）有关的一个小故事

曾经看过一个段子，说一个运维发现一个很奇怪的事情，每年到3 4月份都会有奇怪的流量高峰，调查了好久都查不出来问题。最后发现，因为这会儿是研究生快要毕业的时候，大家都在赶论文，然后好多人就需要数据啊，需要数据怎么办呢，就用爬虫爬取网站数据，造成了这个奇怪的流量高峰。

## 爬虫技术和反爬虫技术

在测试和实验的过程中，发现能够单纯用`urllib2`库就能爬下来的网站太少了，基本都要至少把请求头改成正常的才能爬下来网页。更有甚者，我在一次测试的时候尝试去爬500px，没改头就直接运行了，结果直接被封IP了，后来连正常访问也访问不了，简直丧心病狂。

## My little crawl

进入正题，其实我写爬虫的初衷就是想从网上获得带label的图片数据，想了半天，觉得直接爬搜索引擎比较方便，而且还会有奇怪的干扰数据，正好符合我的需求。

其实我的需求很简单，就是能够快速获得一定数量带label的图片，所以我建了一个`config`文件，利用python自带的`ConfigParser`库来读取配置文件。在读取配置文件的时候遇见了一个问题，就是我想通过配置文件传进去一个我想要的`label`的`list`，但是死活传都是传进去的字符串，最后用了`split(',')`这个方法算是分割成功。

这次写爬虫，发现了一个神器：[selenium](https://github.com/SeleniumHQ/selenium)

这个东西可以模拟浏览器的行为，这对付反爬虫简直不要太方便。不过还是有bug的，比如浏览器的版本在`Firefox 46`以上的就会出现[问题](https://github.com/SeleniumHQ/selenium/issues/2645)，不过总的来说还是很好用的。

> Talk is cheap, show me the code. --Linus Torvalds

-- Here is my [code](https://github.com/firiceguo/get-img).
