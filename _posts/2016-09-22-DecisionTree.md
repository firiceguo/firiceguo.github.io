---
layout: post
title: "Decision Tree"
date: 2016-09-22 21:33
tags: 
    - DataMining
    - MachineLearning
categories: 
    - DataMining
    - MachineLearning
thumbnail:  book
comments: true
description: "决策树，随机森林，GBDT"
---

## 决策树算法

决策树的构建是一个自上而下的分治贪心方法。

- 一开始所有的训练集都在根节点；

- 属性都是明确的；

- 训练集根据选择的属性分区递归；

- 测试属性根据启发式算法或者统计测量选取。

停止分类的情况：

- 所有的训练数据都属于同一类；

- 没有多余的属性让其继续分类；

- 没有数据了。

## 三种方法 —— 用于选取分类的属性

1. 信息熵 Entropy（ID3算法）：

    $$ Info(P)=-\sum_{i=1}^n p_i \log(p_i) $$

    $$ Gain(A) = 1-Info(D) $$

    熵越小，我们得到的信息越多.因此，在选取属性的时候，挑$$ Gain(A) $$大的属性。

    > ID3算法存在一个问题，就是偏向于多值属性，例如，如果存在唯一标识属性ID，则ID3会选择它作为分裂属性，这样虽然使得划分充分纯净，但这种划分对分类几乎毫无用处。

2. 增益率 Gain Ratio （C4.5算法）

    C4.5算法使用增益率来克服ID3算法偏向多值属性的缺点（正则化信息增益）。

    但是可能会对可取值数目较小的属性有所偏好，因此C4.5算法使用了启发式算法：先从候选划分属性中找出**信息增益**高于平均水平的属性，再从中选择**增益率**最高的。

    $$ Splitinfo_A(D)=-\sum_{j=1}^v \frac{|D_j|}{|D|} * \log_2(\frac{|D_j|}{|D|}) $$

    $$ GainRatio(A) = \frac{Gain(A)}{SplitInfo(A)} $$

3. 基尼系数 Gini Index （CART）

    基尼系数定义为：

    $$ gini(D)=1-\sum_{j=1}^n p_j^2 $$

    其中$$p_j$$是$$j$$在$$D$$中的相对频率

    如果$$D$$被分裂成两个子集$$D_1$$和$$D_2$$，此时的基尼系数为：

    $$ gini_A(D)=\frac{|D_1|}{|D|}gini(D_1)+\frac{|D_2|}{|D|}gini(D_2) $$

    拥有最小$$gini_{split}(D)$$的值的属性被选为分裂属性。

### 三种方法的缺点

1. ID3算法存在的问题就是划分的时候会偏向于多值属性。

2. C4.5算法当一部分小于另一部分时偏向于不平衡划分。

3. 基尼系数方法同样偏向于多值属性，同时当类别很多的时候划分困难。这种方法往往有利于两个分区大小接近并且在每个分区都很纯净的测试数据。

## 过拟合(Overfitting)和剪枝(Tree Pruning)

当出现干扰数据或者噪声时，会导致出现很多很深的分支，就是出现过拟合。

可以选择一个**阈值**来使决策树停止生成，此时的阈值很难选。

或者在树完全生成之后**逐步剪掉**多余的分支。此时需要使用和训练集**完全不同**的测试集来测试那种剪枝方法是结果最好的。

- 预剪枝

    在决策树生成的过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来泛化能力的提升，则停止划分。

- 后剪枝

    先生成一个完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能提高泛化能力，则将该结点替换为叶结点。

> 利用验证集来测试泛化能力。

后剪枝通常比预剪枝保留了更多的分支。一般情况下，后剪枝的欠拟合风险更小，泛化能力高于预剪枝。但是后剪枝的训练时间的开销会更大。

## 随机森林

先通过可放回采样，得到T个含m个训练样本的采样集。对于每个采样集训练一个决策树的基学习器。（Bagging）

对基学习器中的每个结点，通过可放回采样，选取k个属性（一共d个属性）进行划分。

k值一般为$$k=log_2 d$$ [Breiman, 2001a]

### 组合策略

平均法，加权平均法，投票法，加权投票法。

> 在基学习器的个体差异较大时使用加权。

## GBDT（Boosting）

> 提升树是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差 = 真实值 - 预测值 。提升树即是整个迭代过程生成的回归树的累加。

[梯度提升树(GBDT)原理小结](http://www.cnblogs.com/pinard/p/6140514.html)

[GBDT算法原理深入解析](https://www.zybuluo.com/yxd/note/611571)
