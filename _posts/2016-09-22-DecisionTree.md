---
layout: post
title: "Decision Tree Induction"
date: 2016-09-22 21:33
tags: 
    - DataMining
    - MachineLearning
categories: 
    - DataMining
    - MachineLearning
thumbnail:  book
description: Decision Tree Induction algorithm
---

## 决策树算法

决策树的构建是一个自上而下的分治贪心方法。

- 一开始所有的训练集都在根节点；

- 属性都是明确的；

- 训练集根据选择的属性分区递归；

- 测试属性根据启发式算法或者统计测量选取。

停止分类的情况：

- 所有的训练数据都属于同一类；

- 没有多余的属性让其继续分类；

- 没有数据了。

## 三种方法 —— 用于选取分类的属性

- 信息熵 Entropy（ID3算法）：

$$ Info(P)=-\sum_{i=1}^n p_i \log(p_i) $$

$$ Gain(A) = 1-Info(D) $$

熵越小，我们得到的信息越多

因此，在选取属性的时候，挑$$ Gain(A) $$大的属性。

> ID3算法存在一个问题，就是偏向于多值属性，例如，如果存在唯一标识属性ID，则ID3会选择它作为分裂属性，这样虽然使得划分充分纯净，但这种划分对分类几乎毫无用处。

- 增益率 Gain Ratio （C4.5算法）

C4.5算法使用增益率来克服ID3算法偏向多值属性的缺点（正则化信息增益）

$$ Splitinfo_A(D)=-\sum_{j=1}^v \frac{|D_j|}{|D|} * \log_2(\frac{|D_j|}{|D|}) $$

$$ GainRatio(A) = \frac{Gain(A)}{SplitInfo(A)} $$

增益率最大的那一项将会被选为分裂属性。

- 基尼系数 Gini Index

基尼系数定义为：

$$ gini(D)=1-\sum_{j=1}^n p_j^2 $$

其中$$p_j$$是$$j$$在$$D$$中的相对频率

如果$$D$$被分裂成两个子集$$D_1$$和$$D_2$$，此时的基尼系数为：

$$ gini_A(D)=\frac{|D_1|}{|D|}gini(D_1)+\frac{|D_2|}{|D|}gini(D_2) $$

拥有最小$$gini_{split}(D)$$的值的属性被选为分裂属性。

### 三种方法的缺点

1. ID3算法存在的问题就是划分的时候会偏向于多值属性。

2. C4.5算法当一部分小于另一部分时偏向于不平衡划分。

3. 基尼系数方法同样偏向于多值属性，同时当类别很多的时候划分困难。这种方法往往有利于两个分区大小接近并且在每个分区都很纯净的测试数据。

## 过拟合(Overfitting)和剪枝(Tree Pruning)

当出现干扰数据或者噪声时，会导致出现很多很深的分支，就是出现过拟合。

可以选择一个**阈值**来使决策树停止生成，此时的阈值很难选。

或者在树完全生成之后**逐步剪掉**多余的分支。此时需要使用和训练集**完全不同**的测试集来测试那种剪枝方法是结果最好的。

## 扩展：雨林算法（RainForest） —— 建立AVC列表

只关注本身的属性和决策值。建立本身属性和决策值的对应表（AVC表）

> 实时性差。

